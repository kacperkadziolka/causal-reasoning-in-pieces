{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-22T15:25:18.110308Z",
     "start_time": "2025-03-22T15:25:18.107767Z"
    }
   },
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm \n",
    "    \n",
    "from datasets import load_dataset\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load the dataset and access the train/validation/test splits",
   "id": "800c8494f0e643ff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T15:25:22.242775Z",
     "start_time": "2025-03-22T15:25:20.547756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Download the CORR2CAUSE dataset\n",
    "dataset_name = \"causal-nlp/corr2cause\"\n",
    "try:\n",
    "    dataset = load_dataset(dataset_name)\n",
    "    print(\"Dataset successfully loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading the dataset: {e}\")"
   ],
   "id": "85ec003a25bf94",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset successfully loaded.\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T15:25:24.249610Z",
     "start_time": "2025-03-22T15:25:23.310241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Access the train, test, and validation splits\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "validation_dataset = dataset['validation']\n",
    "\n",
    "# Convert to Pandas DataFrames\n",
    "train_df = train_dataset.to_pandas()\n",
    "test_df = test_dataset.to_pandas()\n",
    "validation_df = validation_dataset.to_pandas()\n",
    "\n",
    "# Display the length of each split\n",
    "print(f\"Train split length: {len(train_df)}\")\n",
    "print(f\"Test split length: {len(test_df)}\")\n",
    "print(f\"Validation split length: {len(validation_df)}\")"
   ],
   "id": "bf3685ff0cdf4773",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train split length: 205734\n",
      "Test split length: 1162\n",
      "Validation split length: 1076\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T12:27:40.656538Z",
     "start_time": "2025-02-05T12:27:40.632932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Extract only the problems with x  variables\n",
    "num_variables = 6\n",
    "train_df = train_df[train_df['num_variables'] == num_variables]\n",
    "print(f\"Train split length only with {num_variables} variables: {len(train_df)}\")\n"
   ],
   "id": "faf4c8cd8fe38857",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train split length only with 6 variables: 197634\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Parse the input to the adjacency graph format",
   "id": "4c8251a1e8cd672"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T13:34:30.607369Z",
     "start_time": "2025-03-22T13:34:30.585879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def parse_input(text):\n",
    "    # Initialize containers\n",
    "    variables = set()\n",
    "    correlations = []\n",
    "    marginal_independencies = []\n",
    "    conditional_independencies = []\n",
    "\n",
    "    # Extract variables\n",
    "    var_match = re.search(r'variables?(.*?)[\\.\\n]', text)\n",
    "    if var_match:\n",
    "        vars_text = var_match.group(1)\n",
    "        # Split vars_text by commas and 'and'\n",
    "        vars_list = re.split(r',\\s*|\\s+and\\s+|\\s*,\\s*', vars_text)\n",
    "        vars_list = [var.strip() for var in vars_list if var.strip() and var.strip().lower() != 'and']\n",
    "        variables.update(vars_list)\n",
    "    else:\n",
    "        # Handle case when variables not found\n",
    "        pass\n",
    "\n",
    "    # Extract correlations\n",
    "    # The correlations are in the text after 'All the statistical relations among these variables are as follows:'\n",
    "    # and before 'However,'\n",
    "    correlations_text_match = re.search(r'All the statistical relations.*?are as follows:(.*?)(However|$)', text, re.DOTALL)\n",
    "    if correlations_text_match:\n",
    "        correlations_text = correlations_text_match.group(1)\n",
    "        # Now we need to extract all 'X correlates with Y.'\n",
    "        correlation_matches = re.findall(r'([A-Za-z]+) correlates with ([A-Za-z]+)\\.', correlations_text)\n",
    "        correlations.extend(correlation_matches)\n",
    "        variables.update([var for pair in correlation_matches for var in pair])\n",
    "\n",
    "    # Extract independencies\n",
    "    # The independencies are in the text after 'However,'\n",
    "    independencies_text_match = re.search(r'However,(.*)', text, re.DOTALL)\n",
    "    if independencies_text_match:\n",
    "        independencies_text = independencies_text_match.group(1)\n",
    "        # Now split independencies into sentences\n",
    "        sentences = re.findall(r'([^.]*?\\.)', independencies_text)\n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            # Try to match marginal independencies\n",
    "            marg_match = re.match(r'([A-Za-z]+) is independent of ([A-Za-z]+)\\.', sentence)\n",
    "            if marg_match:\n",
    "                var1 = marg_match.group(1)\n",
    "                var2 = marg_match.group(2)\n",
    "                marginal_independencies.append((var1, var2))\n",
    "                variables.update([var1, var2])\n",
    "            else:\n",
    "                # Try to match conditional independencies\n",
    "                cond_match = re.match(r'([A-Za-z]+) and ([A-Za-z]+) are independent given (.*?)[\\.\\n]', sentence)\n",
    "                if cond_match:\n",
    "                    var1 = cond_match.group(1)\n",
    "                    var2 = cond_match.group(2)\n",
    "                    given_vars_text = cond_match.group(3)\n",
    "                    # Split given_vars_text by commas and 'and', strip spaces\n",
    "                    given_vars = re.split(r',\\s*|\\s+and\\s+', given_vars_text)\n",
    "                    given_vars = [var.strip() for var in given_vars if var.strip() and var.strip().lower() != 'and']\n",
    "                    conditional_independencies.append({\n",
    "                        'vars': (var1, var2),\n",
    "                        'given': given_vars\n",
    "                    })\n",
    "                    variables.update([var1, var2] + given_vars)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return {\n",
    "        'variables': list(sorted(variables)),\n",
    "        'correlations': correlations,\n",
    "        'marginal_independencies': marginal_independencies,\n",
    "        'conditional_independencies': conditional_independencies\n",
    "    }\n",
    "\n",
    "def construct_causal_skeleton_with_steps(parsed_data):\n",
    "    variables = parsed_data['variables']\n",
    "    correlations = parsed_data['correlations']\n",
    "    marginal_independencies = parsed_data['marginal_independencies']\n",
    "    conditional_independencies = parsed_data['conditional_independencies']\n",
    "\n",
    "    reasoning_steps = []\n",
    "\n",
    "    # Step 1: Read the Data\n",
    "    reasoning_steps.append(\"Step 1: Read the Data\")\n",
    "    reasoning_steps.append(f\"- Extracted variables: {', '.join(variables)}\")\n",
    "    correlations_str = ', '.join([f\"({var1}, {var2})\" for var1, var2 in correlations])\n",
    "    reasoning_steps.append(f\"- Correlations: {correlations_str}\")\n",
    "    marg_indep_str = ', '.join([f\"({var1}, {var2})\" for var1, var2 in marginal_independencies])\n",
    "    reasoning_steps.append(f\"- Marginal Independencies: {marg_indep_str}\")\n",
    "    cond_indep_str = '\\n  - '.join([\n",
    "        f\"({indep['vars'][0]}, {indep['vars'][1]}) are independent given {', '.join(indep['given'])}\"\n",
    "        for indep in conditional_independencies\n",
    "    ])\n",
    "    reasoning_steps.append(f\"- Conditional Independencies:\\n  - {cond_indep_str}\")\n",
    "\n",
    "    # Step 2: Initialize the Graph\n",
    "    reasoning_steps.append(\"Step 2: Initialize the Graph\")\n",
    "    edges = set()\n",
    "    for var1, var2 in correlations:\n",
    "        edges.add(frozenset([var1, var2]))\n",
    "    initial_edges_str = ', '.join([f\"({var1}, {var2})\" for var1, var2 in correlations])\n",
    "    reasoning_steps.append(f\"- Created edges between all correlated variable pairs.\")\n",
    "    reasoning_steps.append(f\"- Initial edges: {{{initial_edges_str}}}\")\n",
    "\n",
    "    # Step 3: Apply Marginal Independencies\n",
    "    reasoning_steps.append(\"Step 3: Apply Marginal Independencies\")\n",
    "    removed_edges = set()\n",
    "    for var1, var2 in marginal_independencies:\n",
    "        edge = frozenset([var1, var2])\n",
    "        if edge in edges:\n",
    "            edges.remove(edge)\n",
    "            removed_edges.add(edge)\n",
    "            reasoning_steps.append(f\"- **Because {var1} is independent of {var2}, there is no edge between {var1} and {var2}.**\")\n",
    "    if not removed_edges:\n",
    "        reasoning_steps.append(\"- No edges removed in this step.\")\n",
    "\n",
    "    # Step 4: Apply Conditional Independencies\n",
    "    reasoning_steps.append(\"Step 4: Apply Conditional Independencies\")\n",
    "    removed_edges_cond = set()\n",
    "    for indep in conditional_independencies:\n",
    "        var1, var2 = indep['vars']\n",
    "        edge = frozenset([var1, var2])\n",
    "        if edge in edges:\n",
    "            edges.remove(edge)\n",
    "            removed_edges_cond.add(edge)\n",
    "            given_vars_str = ', '.join(indep['given'])\n",
    "            reasoning_steps.append(f\"- Because {var1} and {var2} are independent given {given_vars_str}, there is no edge between {var1} and {var2}.\")\n",
    "    if not removed_edges_cond:\n",
    "        reasoning_steps.append(\"- No edges removed in this step.\")\n",
    "    \n",
    "\n",
    "    # Step 5: Compile the Remaining Edges\n",
    "    reasoning_steps.append(\"Step 5: Compile the Causal Undirected Skeleton\")\n",
    "    remaining_edges = [(list(edge)[0], list(edge)[1]) for edge in edges]\n",
    "    remaining_edges_str = ', '.join([f\"({var1}, {var2})\" for var1, var2 in remaining_edges])\n",
    "    reasoning_steps.append(f\"  - Edges: {{{remaining_edges_str}}}\")\n",
    "\n",
    "    answer = '\\n'.join(reasoning_steps)\n",
    "    return answer\n",
    "\n",
    "def generate_causal_skeleton_reasoning(text):\n",
    "    parsed_data = parse_input(text)\n",
    "    answer = construct_causal_skeleton_with_steps(parsed_data)\n",
    "    return answer"
   ],
   "id": "f0fa3399b55867c5",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Parse the input data to the incident graph format",
   "id": "455566710d39944e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T13:34:35.125599Z",
     "start_time": "2025-03-22T13:34:35.114502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_correlation_adjacency(correlations):\n",
    "    \"\"\"\n",
    "    Builds an adjacency dictionary for correlations.\n",
    "\n",
    "    :param correlations: List of tuples representing correlated node pairs.\n",
    "    :return: Dictionary mapping each node to a set of correlated nodes.\n",
    "    \"\"\"\n",
    "    adjacency = defaultdict(set)\n",
    "    for var1, var2 in correlations:\n",
    "        adjacency[var1].add(var2)\n",
    "        adjacency[var2].add(var1)\n",
    "    return adjacency\n",
    "\n",
    "\n",
    "def format_correlation_adjacency(adjacency):\n",
    "    \"\"\"\n",
    "    Formats the adjacency dictionary into a readable string.\n",
    "\n",
    "    :param adjacency: Dictionary mapping each node to a set of correlated nodes.\n",
    "    :return: Formatted string representing the correlations.\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    for node in sorted(adjacency.keys()):\n",
    "        connected_nodes = sorted(adjacency[node])\n",
    "        if connected_nodes:\n",
    "            # Use 'node' or 'nodes' based on the number of connected nodes\n",
    "            node_word = \"node\" if len(connected_nodes) == 1 else \"nodes\"\n",
    "            connected_str = ', '.join(connected_nodes)\n",
    "            lines.append(f\"Node {node} is correlated with {node_word} {connected_str}.\")\n",
    "        else:\n",
    "            lines.append(f\"Node {node} has no correlations.\")\n",
    "    return '\\n  - '.join(lines)\n",
    "\n",
    "\n",
    "def format_adjacency(adjacency):\n",
    "    lines = []\n",
    "    for var in sorted(adjacency.keys()):\n",
    "        connected_nodes = sorted(adjacency[var])\n",
    "        if connected_nodes:\n",
    "            # Determine whether to use 'node' or 'nodes'\n",
    "            node_word = \"node\" if len(connected_nodes) == 1 else \"nodes\"\n",
    "            connected_str = ', '.join(connected_nodes)\n",
    "            lines.append(f\"Node {var} is connected to {node_word} {connected_str}.\")\n",
    "        else:\n",
    "            lines.append(f\"Node {var} has no connections.\")\n",
    "    return '\\n  - '.join(lines)\n",
    "\n",
    "\n",
    "def construct_incident_causal_skeleton_with_steps(parsed_data):\n",
    "    variables = parsed_data['variables']\n",
    "    correlations = parsed_data['correlations']\n",
    "    marginal_independencies = parsed_data['marginal_independencies']\n",
    "    conditional_independencies = parsed_data['conditional_independencies']\n",
    "\n",
    "    reasoning_steps = []\n",
    "\n",
    "    # Step 1: Read the Data\n",
    "    reasoning_steps.append(\"Step 1: Read the Data\")\n",
    "    reasoning_steps.append(f\"- Extracted nodes: {', '.join(variables)}\")\n",
    "    \n",
    "    # Build and format the correlation adjacency\n",
    "    correlation_adjacency = build_correlation_adjacency(correlations)\n",
    "    formatted_correlations = format_correlation_adjacency(correlation_adjacency)\n",
    "    reasoning_steps.append(\"- Correlations:\")\n",
    "    reasoning_steps.append(f\"  - {formatted_correlations}\")\n",
    "    \n",
    "    # Marginal Independencies\n",
    "    if marginal_independencies:\n",
    "        reasoning_steps.append(\"- Marginal Independencies:\")\n",
    "        for var1, var2 in marginal_independencies:\n",
    "            reasoning_steps.append(f\"  - Node {var1} is independent of node {var2}.\")\n",
    "    else:\n",
    "        reasoning_steps.append(\"- Marginal Independencies: None\")\n",
    "    \n",
    "    # Conditional Independencies\n",
    "    if conditional_independencies:\n",
    "        cond_indep_str = '\\n  - '.join([\n",
    "            f\"Nodes {indep['vars'][0]} and {indep['vars'][1]} are independent given \"\n",
    "            f\"{'node' if len(indep['given']) == 1 else 'nodes'} {', '.join(indep['given'])}\"\n",
    "            for indep in conditional_independencies\n",
    "        ])\n",
    "        reasoning_steps.append(f\"- Conditional Independencies:\\n  - {cond_indep_str}\")\n",
    "    else:\n",
    "        reasoning_steps.append(f\"- Conditional Independencies: None\")\n",
    "\n",
    "    # Step 2: Initialize the Graph\n",
    "    reasoning_steps.append(\"\\nStep 2: Initialize the Graph\")\n",
    "    edges = set()\n",
    "    for var1, var2 in correlations:\n",
    "        edges.add(frozenset([var1, var2]))\n",
    "    \n",
    "    # Create adjacency list after initialization\n",
    "    adjacency = {var: set() for var in variables}\n",
    "    for edge in edges:\n",
    "        var1, var2 = sorted(edge)  # Sort for consistent ordering\n",
    "        adjacency[var1].add(var2)\n",
    "        adjacency[var2].add(var1)\n",
    "    \n",
    "    # Format adjacency list\n",
    "    adjacency_str = format_adjacency(adjacency)\n",
    "    reasoning_steps.append(\"Created edges between all correlated variable pairs. In this graph:\")\n",
    "    reasoning_steps.append(f\"  - {adjacency_str}\")\n",
    "\n",
    "    # Step 3: Apply Marginal Independencies\n",
    "    reasoning_steps.append(\"\\nStep 3: Apply Marginal Independencies\")\n",
    "    removed_edges = set()\n",
    "    for var1, var2 in marginal_independencies:\n",
    "        edge = frozenset([var1, var2])\n",
    "        if edge in edges:\n",
    "            edges.remove(edge)\n",
    "            removed_edges.add(edge)\n",
    "            reasoning_steps.append(f\"- **Because {var1} is independent of {var2}, there is no edge between {var1} and {var2}.**\")\n",
    "    if not removed_edges:\n",
    "        reasoning_steps.append(\"- No edges removed in this step.\")\n",
    "\n",
    "    # Step 4: Apply Conditional Independencies\n",
    "    reasoning_steps.append(\"\\nStep 4: Apply Conditional Independencies\")\n",
    "    removed_edges_cond = set()\n",
    "    for indep in conditional_independencies:\n",
    "        var1, var2 = indep['vars']\n",
    "        edge = frozenset([var1, var2])\n",
    "        if edge in edges:\n",
    "            edges.remove(edge)\n",
    "            removed_edges_cond.add(edge)\n",
    "            given_vars = indep['given']\n",
    "            given_vars_str = ', '.join(indep['given'])\n",
    "            node_word = \"node\" if len(given_vars) == 1 else \"nodes\"\n",
    "            reasoning_steps.append(f\"- Because nodes {var1} and {var2} are independent given {node_word} {given_vars_str}, there is no edge between nodes {var1} and {var2}.\")\n",
    "    if not removed_edges_cond:\n",
    "        reasoning_steps.append(\"- No edges removed in this step.\")\n",
    "\n",
    "    # Step 5: Compile the Causal Undirected Skeleton in Incident Form\n",
    "    # reasoning_steps.append(\"\\nAnswer:\")\n",
    "    reasoning_steps.append(\"\\nStep 5: Compile the Causal Undirected Skeleton\")\n",
    "    \n",
    "    # Initialize adjacency list\n",
    "    adjacency = {var: set() for var in variables}\n",
    "    \n",
    "    # Populate adjacency list based on remaining edges\n",
    "    for edge in edges:\n",
    "        var1, var2 = sorted(edge)  # Ensure consistent ordering\n",
    "        adjacency[var1].add(var2)\n",
    "        adjacency[var2].add(var1)\n",
    "    \n",
    "    # Create incident form string with grammatical consistency\n",
    "    incident_form = []\n",
    "    for var in sorted(adjacency.keys()):\n",
    "        connected_nodes = sorted(adjacency[var])\n",
    "        if connected_nodes:\n",
    "            # Determine whether to use 'node' or 'nodes'\n",
    "            node_word = \"node\" if len(connected_nodes) == 1 else \"nodes\"\n",
    "            connected_str = ', '.join(connected_nodes)\n",
    "            incident_form.append(f\"Node {var} is connected to {node_word} {connected_str}.\")\n",
    "        else:\n",
    "            incident_form.append(f\"Node {var} has no connections.\")\n",
    "    \n",
    "    # Append to reasoning steps\n",
    "    reasoning_steps.append(\"In this graph:\")\n",
    "    for line in incident_form:\n",
    "        reasoning_steps.append(f\"  - {line}\")\n",
    "    \n",
    "    answer = '\\n'.join(reasoning_steps)\n",
    "    return answer"
   ],
   "id": "634c909c1a945f6f",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T13:34:39.772585Z",
     "start_time": "2025-03-22T13:34:39.769811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_incident_causal_skeleton_reasoning(text):\n",
    "    parsed_data = parse_input(text)\n",
    "    answer = construct_incident_causal_skeleton_with_steps(parsed_data)\n",
    "    return answer"
   ],
   "id": "a2bc45045ba02c41",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T13:34:41.379071Z",
     "start_time": "2025-03-22T13:34:41.364792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def construct_incident_causal_skeleton_only_answer(parsed_data):\n",
    "    variables = parsed_data['variables']\n",
    "    correlations = parsed_data['correlations']\n",
    "    marginal_independencies = parsed_data['marginal_independencies']\n",
    "    conditional_independencies = parsed_data['conditional_independencies']\n",
    "\n",
    "    reasoning_steps = []\n",
    "\n",
    "    # Step 1: Read the Data\n",
    "    reasoning_steps.append(\"Step 1: Read the Data\")\n",
    "    reasoning_steps.append(f\"- Extracted nodes: {', '.join(variables)}\")\n",
    "\n",
    "    # Build and format the correlation adjacency\n",
    "    correlation_adjacency = build_correlation_adjacency(correlations)\n",
    "    formatted_correlations = format_correlation_adjacency(correlation_adjacency)\n",
    "    reasoning_steps.append(\"- Correlations:\")\n",
    "    reasoning_steps.append(f\"  - {formatted_correlations}\")\n",
    "\n",
    "    # Marginal Independencies\n",
    "    if marginal_independencies:\n",
    "        reasoning_steps.append(\"- Marginal Independencies:\")\n",
    "        for var1, var2 in marginal_independencies:\n",
    "            reasoning_steps.append(f\"  - Node {var1} is independent of node {var2}.\")\n",
    "    else:\n",
    "        reasoning_steps.append(\"- Marginal Independencies: None\")\n",
    "\n",
    "    # Conditional Independencies\n",
    "    if conditional_independencies:\n",
    "        cond_indep_str = '\\n  - '.join([\n",
    "            f\"Nodes {indep['vars'][0]} and {indep['vars'][1]} are independent given \"\n",
    "            f\"{'node' if len(indep['given']) == 1 else 'nodes'} {', '.join(indep['given'])}\"\n",
    "            for indep in conditional_independencies\n",
    "        ])\n",
    "        reasoning_steps.append(f\"- Conditional Independencies:\\n  - {cond_indep_str}\")\n",
    "    else:\n",
    "        reasoning_steps.append(f\"- Conditional Independencies: None\")\n",
    "\n",
    "    # Step 2: Initialize the Graph\n",
    "    reasoning_steps.append(\"\\nStep 2: Initialize the Graph\")\n",
    "    edges = set()\n",
    "    for var1, var2 in correlations:\n",
    "        edges.add(frozenset([var1, var2]))\n",
    "\n",
    "    # Create adjacency list after initialization\n",
    "    adjacency = {var: set() for var in variables}\n",
    "    for edge in edges:\n",
    "        var1, var2 = sorted(edge)  # Sort for consistent ordering\n",
    "        adjacency[var1].add(var2)\n",
    "        adjacency[var2].add(var1)\n",
    "\n",
    "    # Format adjacency list\n",
    "    adjacency_str = format_adjacency(adjacency)\n",
    "    reasoning_steps.append(\"Created edges between all correlated variable pairs. In this graph:\")\n",
    "    reasoning_steps.append(f\"  - {adjacency_str}\")\n",
    "\n",
    "    # Step 3: Apply Marginal Independencies\n",
    "    reasoning_steps.append(\"\\nStep 3: Apply Marginal Independencies\")\n",
    "    removed_edges = set()\n",
    "    for var1, var2 in marginal_independencies:\n",
    "        edge = frozenset([var1, var2])\n",
    "        if edge in edges:\n",
    "            edges.remove(edge)\n",
    "            removed_edges.add(edge)\n",
    "            reasoning_steps.append(f\"- **Because {var1} is independent of {var2}, there is no edge between {var1} and {var2}.**\")\n",
    "    if not removed_edges:\n",
    "        reasoning_steps.append(\"- No edges removed in this step.\")\n",
    "\n",
    "    # Step 4: Apply Conditional Independencies\n",
    "    reasoning_steps.append(\"\\nStep 4: Apply Conditional Independencies\")\n",
    "    removed_edges_cond = set()\n",
    "    for indep in conditional_independencies:\n",
    "        var1, var2 = indep['vars']\n",
    "        edge = frozenset([var1, var2])\n",
    "        if edge in edges:\n",
    "            edges.remove(edge)\n",
    "            removed_edges_cond.add(edge)\n",
    "            given_vars = indep['given']\n",
    "            given_vars_str = ', '.join(indep['given'])\n",
    "            node_word = \"node\" if len(given_vars) == 1 else \"nodes\"\n",
    "            reasoning_steps.append(f\"- Because nodes {var1} and {var2} are independent given {node_word} {given_vars_str}, there is no edge between nodes {var1} and {var2}.\")\n",
    "    if not removed_edges_cond:\n",
    "        reasoning_steps.append(\"- No edges removed in this step.\")\n",
    "\n",
    "    \"\"\"\n",
    "    Refactor the below code, as to not have the following structure:\n",
    "    Step 1. Read data\n",
    "    Step 2. Extract variables\n",
    "    Step 3. Compute independencies\n",
    "    Step 4. Initialize graph\n",
    "    Step 5. Apply independencies and provide computed causal graph\n",
    "\n",
    "    Rather than that, for that particular experiment, we would like the model to provide it with:\n",
    "    - Premise and actual question (e.g. \"What is the causal undirected skeleton?\")\n",
    "    - The expected answer (e.g. \"The causal undirected skeleton is ...\")\n",
    "    Therefore not provide the model with reasoning steps.\n",
    "    This could be particularly useful for later experiments done with reasoning models.\n",
    "    \"\"\"\n",
    "    # This is the hacky way to do it, rather than refactoring the code above\n",
    "    # We just overwrite the reasoning 'output' but still compute the causal skeleton step by step\n",
    "    reasoning_steps = []\n",
    "\n",
    "    # Computed Causal Undirected Skeleton in Incident Form\n",
    "    # reasoning_steps.append(\"\\nAnswer:\")\n",
    "    reasoning_steps.append(\"Computed Causal Undirected Skeleton:\")\n",
    "\n",
    "    # Initialize adjacency list\n",
    "    adjacency = {var: set() for var in variables}\n",
    "\n",
    "    # Populate adjacency list based on remaining edges\n",
    "    for edge in edges:\n",
    "        var1, var2 = sorted(edge)  # Ensure consistent ordering\n",
    "        adjacency[var1].add(var2)\n",
    "        adjacency[var2].add(var1)\n",
    "\n",
    "    # Create incident form string with grammatical consistency\n",
    "    incident_form = []\n",
    "    for var in sorted(adjacency.keys()):\n",
    "        connected_nodes = sorted(adjacency[var])\n",
    "        if connected_nodes:\n",
    "            # Determine whether to use 'node' or 'nodes'\n",
    "            node_word = \"node\" if len(connected_nodes) == 1 else \"nodes\"\n",
    "            connected_str = ', '.join(connected_nodes)\n",
    "            incident_form.append(f\"Node {var} is connected to {node_word} {connected_str}.\")\n",
    "        else:\n",
    "            incident_form.append(f\"Node {var} has no connections.\")\n",
    "\n",
    "    # Append to reasoning steps\n",
    "    # reasoning_steps.append(\"In this graph:\")\n",
    "    for line in incident_form:\n",
    "        reasoning_steps.append(f\"  - {line}\")\n",
    "\n",
    "    answer = '\\n'.join(reasoning_steps)\n",
    "    return answer"
   ],
   "id": "b8a4f8418e69898d",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T13:34:47.247705Z",
     "start_time": "2025-03-22T13:34:47.245321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_incident_causal_skeleton_only_answer(text):\n",
    "    parsed_data = parse_input(text)\n",
    "    answer = construct_incident_causal_skeleton_only_answer(parsed_data)\n",
    "    return answer"
   ],
   "id": "fa2e5a2e0674e8d4",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test processing function and re-format whole datasets",
   "id": "f8dc2a067779828e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T13:34:51.841683Z",
     "start_time": "2025-03-22T13:34:51.838442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_causal_answers(text):\n",
    "    \"\"\"\n",
    "    Generates both multi-line and single-line expected answers.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input text containing the premise.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with 'expected_answer' and 'expected_answer_single_line'.\n",
    "    \"\"\"\n",
    "    # multi_line_answer = generate_incident_causal_skeleton_reasoning(text)\n",
    "    multi_line_answer = generate_incident_causal_skeleton_only_answer(text)\n",
    "    \n",
    "    # Replace actual newlines with literal '\\n' to create a single-line answer\n",
    "    single_line_answer = multi_line_answer.replace('\\n', '\\\\n')\n",
    "    \n",
    "    return {\n",
    "        'expected_answer': multi_line_answer,\n",
    "        'expected_answer_single_line': single_line_answer\n",
    "    }"
   ],
   "id": "d1de65219d3f7684",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T19:58:50.782522Z",
     "start_time": "2025-01-30T19:58:50.775127Z"
    }
   },
   "cell_type": "code",
   "source": "train_df.iloc[0]['input']",
   "id": "31bdb2589143d1c8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Premise: Suppose there is a closed system of 4 variables, A, B, C and D. All the statistical relations among these 4 variables are as follows: A correlates with B. A correlates with C. A correlates with D. B correlates with C. B correlates with D. C correlates with D. However, B and D are independent given A. B and D are independent given A and C. C and D are independent given A. C and D are independent given A and B.\\nHypothesis: There exists at least one collider (i.e., common effect) of A and B.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T19:58:54.935194Z",
     "start_time": "2025-01-30T19:58:54.930918Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result = generate_causal_skeleton_reasoning(train_df.iloc[0]['input'])\n",
    "print(result)"
   ],
   "id": "ce5351b88b54d3e3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Read the Data\n",
      "- Extracted variables: A, B, C, D\n",
      "- Correlations: (A, B), (A, C), (A, D), (B, C), (B, D), (C, D)\n",
      "- Marginal Independencies: \n",
      "- Conditional Independencies:\n",
      "  - (B, D) are independent given A\n",
      "  - (B, D) are independent given A, C\n",
      "  - (C, D) are independent given A\n",
      "  - (C, D) are independent given A, B\n",
      "Step 2: Initialize the Graph\n",
      "- Created edges between all correlated variable pairs.\n",
      "- Initial edges: {(A, B), (A, C), (A, D), (B, C), (B, D), (C, D)}\n",
      "Step 3: Apply Marginal Independencies\n",
      "- No edges removed in this step.\n",
      "Step 4: Apply Conditional Independencies\n",
      "- Because B and D are independent given A, there is no edge between B and D.\n",
      "- Because C and D are independent given A, there is no edge between C and D.\n",
      "Step 5: Compile the Causal Undirected Skeleton\n",
      "  - Edges: {(C, B), (A, C), (A, D), (A, B)}\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T19:59:00.595639Z",
     "start_time": "2025-01-30T19:59:00.592681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result = generate_incident_causal_skeleton_reasoning(train_df.iloc[10]['input'])\n",
    "print(result)"
   ],
   "id": "22d9d74090820f35",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Read the Data\n",
      "- Extracted nodes: A, B, C, D\n",
      "- Correlations:\n",
      "  - Node A is correlated with nodes C, D.\n",
      "  - Node B is correlated with nodes C, D.\n",
      "  - Node C is correlated with nodes A, B, D.\n",
      "  - Node D is correlated with nodes A, B, C.\n",
      "- Marginal Independencies:\n",
      "  - Node A is independent of node B.\n",
      "- Conditional Independencies:\n",
      "  - Nodes A and D are independent given nodes B, C\n",
      "  - Nodes A and D are independent given node C\n",
      "  - Nodes B and D are independent given nodes A, C\n",
      "  - Nodes B and D are independent given node C\n",
      "\n",
      "Step 2: Initialize the Graph\n",
      "Created edges between all correlated variable pairs. In this graph:\n",
      "  - Node A is connected to nodes C, D.\n",
      "  - Node B is connected to nodes C, D.\n",
      "  - Node C is connected to nodes A, B, D.\n",
      "  - Node D is connected to nodes A, B, C.\n",
      "\n",
      "Step 3: Apply Marginal Independencies\n",
      "- No edges removed in this step.\n",
      "\n",
      "Step 4: Apply Conditional Independencies\n",
      "- Because nodes A and D are independent given nodes B, C, there is no edge between nodes A and D.\n",
      "- Because nodes B and D are independent given nodes A, C, there is no edge between nodes B and D.\n",
      "\n",
      "Step 5: Compile the Causal Undirected Skeleton\n",
      "In this graph:\n",
      "  - Node A is connected to node C.\n",
      "  - Node B is connected to node C.\n",
      "  - Node C is connected to nodes A, B, D.\n",
      "  - Node D is connected to node C.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T20:52:32.189668Z",
     "start_time": "2025-01-30T20:52:32.186609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result = generate_incident_causal_skeleton_only_answer(train_df.iloc[10]['input'])\n",
    "print(result)"
   ],
   "id": "441eee3e36d6c515",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed Causal Undirected Skeleton:\n",
      "  - Node A is connected to node C.\n",
      "  - Node B is connected to node C.\n",
      "  - Node C is connected to nodes A, B, D.\n",
      "  - Node D is connected to node C.\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T13:34:59.278955Z",
     "start_time": "2025-03-22T13:34:59.271593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Process the sample input\n",
    "def test_single_row():\n",
    "    test_row = train_df.iloc[0]['input']\n",
    "    result = generate_causal_answers(test_row)\n",
    "    print(\"=== Multi-Line Answer ===\")\n",
    "    print(result['expected_answer'])\n",
    "    print(\"\\n=== Single-Line Answer ===\")\n",
    "    print(result['expected_answer_single_line'])\n",
    "\n",
    "# Run the test\n",
    "test_single_row()"
   ],
   "id": "7b902a4a13f2300",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Multi-Line Answer ===\n",
      "Computed Causal Undirected Skeleton:\n",
      "  - Node A is connected to nodes B, C, D.\n",
      "  - Node B is connected to nodes A, C.\n",
      "  - Node C is connected to nodes A, B.\n",
      "  - Node D is connected to node A.\n",
      "\n",
      "=== Single-Line Answer ===\n",
      "Computed Causal Undirected Skeleton:\\n  - Node A is connected to nodes B, C, D.\\n  - Node B is connected to nodes A, C.\\n  - Node C is connected to nodes A, B.\\n  - Node D is connected to node A.\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T13:35:03.827334Z",
     "start_time": "2025-03-22T13:35:03.823462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_dataframe_with_progress(df, input_column, output_columns, processing_function, output_file):\n",
    "    \"\"\"\n",
    "    Processes a DataFrame by applying a function to a specified input column and saving the result in a new column.\n",
    "    Includes a progress bar to track the processing.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to process.\n",
    "        input_column (str): The name of the input column where the function will be applied.\n",
    "        output_column (str): The name of the new column to store the results.\n",
    "        processing_function (callable): The function to apply to each row's input column.\n",
    "        output_file (str): The file path to save the processed DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The modified DataFrame with the new column added.\n",
    "    \"\"\"\n",
    "    # Make a copy of the original DataFrame\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Enable the tqdm progress bar for pandas\n",
    "    tqdm.pandas(desc=f\"Processing {', '.join(output_columns)}\")\n",
    "\n",
    "    # Apply the processing function with progress tracking\n",
    "    results = df_copy[input_column].progress_apply(processing_function)\n",
    "    \n",
    "    # If the processing function returns a dictionary or Series, expand into multiple columns\n",
    "    results_df = pd.DataFrame(results.tolist(), index=df_copy.index)\n",
    "    \n",
    "    # Assign the new columns\n",
    "    for col in output_columns:\n",
    "        if col in results_df.columns:\n",
    "            df_copy[col] = results_df[col]\n",
    "        else:\n",
    "            df_copy[col] = None  # or some default value\n",
    "\n",
    "    # Save the modified DataFrame to a file, e.g., as a CSV\n",
    "    df_copy.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Processing complete and DataFrame saved to {output_file}.\")\n",
    "    return df_copy"
   ],
   "id": "bb04c2cc91bce94e",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T20:52:36.734852Z",
     "start_time": "2025-01-30T20:52:36.653056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "processed_train_df = process_dataframe_with_progress(\n",
    "    df=train_df,\n",
    "    input_column='input',\n",
    "    # output_columns=['expected_answer', 'expected_answer_single_line'],\n",
    "    output_columns=['expected_answer'],\n",
    "    processing_function=generate_causal_answers,\n",
    "    output_file='v0.0.8/train.csv'\n",
    ")"
   ],
   "id": "426b9a3cb3637424",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Processing expected_answer:   0%|          | 0/576 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6cf9e48ee2be4cd8ba1a1d734e24bd4c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete and DataFrame saved to v0.0.8/train.csv.\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T21:52:06.143354Z",
     "start_time": "2024-11-19T21:52:05.809671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "processed_validation_df = process_dataframe_with_progress(\n",
    "    df=validation_df,\n",
    "    input_column='input',\n",
    "    output_columns=['expected_answer', 'expected_answer_single_line'],\n",
    "    processing_function=generate_causal_answers,\n",
    "    output_file='v0.0.3/validation.csv'\n",
    ")"
   ],
   "id": "d46fa0399ff9d717",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Processing expected_answer, expected_answer_single_line:   0%|          | 0/1076 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dca4d4ae1f5b4a70936ec60cb011066c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete and DataFrame saved to data/v0.0.3/validation.csv.\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T21:52:08.445271Z",
     "start_time": "2024-11-19T21:52:08.139763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "processed_test_df = process_dataframe_with_progress(\n",
    "    df=test_df,\n",
    "    input_column='input',\n",
    "    output_columns=['expected_answer', 'expected_answer_single_line'],\n",
    "    processing_function=generate_causal_answers,\n",
    "    output_file='v0.0.3/test.csv'\n",
    ")"
   ],
   "id": "8b625953777baf1c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Processing expected_answer, expected_answer_single_line:   0%|          | 0/1162 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6218ec06884c4cff9bf24f5425582a78"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete and DataFrame saved to data/v0.0.3/test.csv.\n"
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Prepare the balanced 6-variables test dataset",
   "id": "c1e1bc6042b6dcae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T15:25:29.864577Z",
     "start_time": "2025-03-22T15:25:29.852441Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# test_df = test_dataset.to_pandas()\n",
    "\n",
    "# Extract only the problems with x  variables\n",
    "num_variables = 6\n",
    "test_df = test_df[test_df['num_variables'] == num_variables]\n",
    "print(f\"Test split length only with {num_variables} variables: {len(test_df)}\")"
   ],
   "id": "274bbe362d48ac07",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test split length only with 6 variables: 522\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T15:25:35.018389Z",
     "start_time": "2025-03-22T15:25:35.012487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Count the occurrences of each label value\n",
    "label_counts = test_df['label'].value_counts()\n",
    "label_percentages = test_df['label'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Print the statistics\n",
    "print(f\"Label distribution in test dataset with {num_variables} variables:\")\n",
    "print(f\"0 (False): {label_counts.get(0, 0)} ({label_percentages.get(0, 0):.1f}%)\")\n",
    "print(f\"1 (True): {label_counts.get(1, 0)} ({label_percentages.get(1, 0):.1f}%)\")\n"
   ],
   "id": "3051654adf55737c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution in test dataset with 6 variables:\n",
      "0 (False): 402 (77.0%)\n",
      "1 (True): 120 (23.0%)\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T15:25:44.069923Z",
     "start_time": "2025-03-22T15:25:44.048680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "balanced_test_df = pd.concat([\n",
    "    test_df[test_df['label'] == 1],\n",
    "    test_df[test_df['label'] == 0].sample(n=120, random_state=42)\n",
    "])\n",
    "\n",
    "# Shuffle the dataset\n",
    "test_df = balanced_test_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "label_counts = test_df['label'].value_counts()\n",
    "label_percentages = test_df['label'].value_counts(normalize=True) * 100\n",
    "print(f\"Label distribution in test dataset with {num_variables} variables:\")\n",
    "print(f\"0 (False): {label_counts.get(0, 0)} ({label_percentages.get(0, 0):.1f}%)\")\n",
    "print(f\"1 (True): {label_counts.get(1, 0)} ({label_percentages.get(1, 0):.1f}%)\")"
   ],
   "id": "4c9e4b5b124ea03c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution in test dataset with 6 variables:\n",
      "0 (False): 120 (50.0%)\n",
      "1 (True): 120 (50.0%)\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T15:25:50.161659Z",
     "start_time": "2025-03-22T15:25:50.026538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "processed_balanced_test_df = process_dataframe_with_progress(\n",
    "    df=test_df,\n",
    "    input_column='input',\n",
    "    # output_columns=['expected_answer', 'expected_answer_single_line'],\n",
    "    output_columns=['expected_answer'],\n",
    "    processing_function=generate_causal_answers,\n",
    "    output_file='test/balanced_50_50_test.csv'\n",
    ")"
   ],
   "id": "c918db2555736bf1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Processing expected_answer:   0%|          | 0/240 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4327c6e29a3943b4bc4ec1c59f1ba1e0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete and DataFrame saved to test/balanced_50_50_test.csv.\n"
     ]
    }
   ],
   "execution_count": 34
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
