# Sample 264 Failure Analysis & System Improvement Plan

**Date:** 2025-12-15
**Sample:** Index 264 (PC Algorithm - Causal Discovery)
**Status:** Failed (Predicted: True, Expected: False)
**Root Cause:** Incorrect skeleton construction leading to wrong causal structure

---

## Table of Contents

- [Executive Summary](#executive-summary)
- [Sample 264 Detailed Analysis](#sample-264-detailed-analysis)
- [Systemic Issues (Algorithm-Agnostic)](#systemic-issues-algorithm-agnostic)
- [Improvement Proposals](#improvement-proposals)
- [Implementation Roadmap](#implementation-roadmap)
- [Expected Impact](#expected-impact)

---

## Executive Summary

### The Problem
Sample 264 failed because the **skeleton construction stage** (Stage 1 of PC algorithm) produced an incorrect graph with only 4 edges instead of the proper skeleton. This error cascaded through all subsequent stages, leading to an incorrect final decision.

### Root Cause
The failure stems from a **gap between knowledge extraction and execution**:
1. ‚úÖ Knowledge correctly identifies PC algorithm stages
2. ‚úÖ Planning correctly sequences the stages
3. ‚ö†Ô∏è **Prompt templates lack algorithmic rigor and operational details**
4. ‚ùå **Executor LLM makes algorithmic errors despite correct high-level instructions**

### Core Insight
The pipeline is **algorithm-agnostic by design**, but needs **stronger algorithmic enforcement mechanisms** at the planning and execution levels without compromising its generality.

---

## Sample 264 Detailed Analysis

### Input
- **Variables:** A, B, C, D, E (5 variables)
- **Correlations:** 10 pairs (complete correlation graph)
- **Conditional Independencies:** 40 CI statements
- **Hypothesis:** "A directly causes E"
- **Expected Answer:** False (A does NOT directly cause E)

### Execution Trace

#### Stage 1: Skeleton Identification ‚ùå
**Expected Behavior:**
- Start with complete graph: 10 edges (all pairs)
- Remove edges based on CI tests from premise
- Should result in ~5-7 edges after removals

**Actual Output:**
```json
{
  "nodes": ["A", "B", "C", "D", "E"],
  "edges": [
    {"source": "A", "target": "B"},
    {"source": "A", "target": "E"},  // ‚Üê This edge should be analyzed more carefully
    {"source": "B", "target": "C"},
    {"source": "C", "target": "D"}
  ]
}
```

**Problems:**
1. Only 4 edges (incomplete skeleton)
2. Kept A‚ÄîE edge (likely incorrect given hypothesis is false)
3. Missing many edges that should remain after CI tests
4. No evidence of systematic CI testing

#### Stage 2: V-Structure Identification ‚ö†Ô∏è
**Output:** All edges remained undirected (no colliders found)

**Problem:** With 40 CI statements, colliders should exist. No colliders = wrong skeleton from Stage 1.

#### Stage 3: Edge Orientation Propagation ‚ùå
**Output:** Incorrectly oriented A ‚Üí E (directed)

**Problem:**
- Changed A‚ÄîE from undirected to directed without proper justification
- Meek's rules shouldn't orient this edge without colliders

#### Stage 4: Hypothesis Evaluation ‚ùå
**Output:** `{"decision": true}`

**Problem:**
- Saw A ‚Üí E in CPDAG
- Concluded "A directly causes E" ‚Üí True
- **Correct answer:** False

### Error Cascade

```
Stage 1: Wrong skeleton (4 edges instead of ~6-7)
    ‚Üì
Stage 2: No colliders identified (should have some)
    ‚Üì
Stage 3: Incorrect edge orientation (A ‚Üí E)
    ‚Üì
Stage 4: Wrong final decision (True instead of False)
```

### Why Stage 1 Failed

The prompt said:
> "Construct a complete undirected graph and iteratively remove edges using conditional independence tests"

But it DIDN'T specify:
- Start with exactly **10 edges** (all pairs of 5 variables)
- For EACH edge, check if premise contains "X ‚´´ Y | S"
- Only remove edges with EXPLICIT CI justification
- Systematically test all conditioning set sizes (|S|=0, 1, 2, ...)

The executor LLM interpreted this loosely and skipped edges.

---

## Systemic Issues (Algorithm-Agnostic)

### Issue 1: Prompt Quality is the Bottleneck

**Current State:**
- Prompts focus on **data transformation**
- Missing **algorithmic constraints**
- Missing **mathematical invariants**
- Missing **step-by-step verification**

**Evidence:**
```python
# From multi_agent_planner.py:165
"Keep prompts focused on DATA TRANSFORMATION, not algorithmic theory"
```

**Impact:** For complex algorithms, the executor needs both transformation instructions AND algorithmic guardrails.

---

### Issue 2: No Algorithmic Guardrails in Execution

**Current Executor System Prompt:**
```python
"""
You execute a specific stage of a decomposed reasoning workflow.
Return ONLY valid JSON that matches the given output_schema exactly.
"""
```

**Problems:**
- ‚úÖ Enforces JSON format
- ‚ùå Doesn't enforce algorithmic correctness
- ‚ùå Doesn't verify mathematical invariants
- ‚ùå No self-validation mechanism

---

### Issue 3: Knowledge Extraction Lacks Operational Details

**Current Knowledge Output:**
```
CANONICAL STAGES:
1. Skeleton Identification: Start with complete undirected graph...
```

**Missing:**
- **HOW** to construct complete graph (all n*(n-1)/2 edges)
- **HOW** to check for CI statements (exact matching? pattern?)
- **WHAT** to do with missing information
- **VALIDATION** criteria (e.g., skeleton must have ‚â§ n*(n-1)/2 edges)

---

### Issue 4: No Intermediate Validation

**Current State:**
- ‚úÖ Plan validation (schema alignment, placeholders)
- ‚ùå **No execution validation** (are outputs mathematically valid?)

**Example:**
Stage 1 outputs 4 edges, but there's no check that says:
- "Did you start with 10 edges?"
- "Are 4 edges reasonable for 5 variables with these CI statements?"
- "Did you systematically test all edges?"

---

## Improvement Proposals

### Proposal 1: Enhanced Knowledge Extraction with Algorithmic Constraints

**Goal:** Extract not just stages, but **operational constraints** and **validation criteria**.

**Implementation:**
Add a new agent to knowledge extraction pipeline:
- **Constraints Agent**: Extracts algorithmic invariants, prerequisites, postconditions

**Enhanced Output Format:**
```markdown
## <CANONICAL_STAGES>
1. Skeleton Identification
   **Inputs:** Variables V
   **Process:** Construct complete graph, remove edges via CI tests
   **Outputs:** Skeleton graph

   **Preconditions:**
     - None

   **Postconditions:**
     - Skeleton has |V| nodes
     - Skeleton has ‚â§ |V|*(|V|-1)/2 edges
     - All edges in skeleton had marginal correlation in input
     - All removed edges have recorded separating sets

   **Invariants:**
     - Skeleton is undirected
     - No self-loops
     - No duplicate edges

   **Validation Criteria:**
     - Started with complete graph
     - Tested all edges systematically
     - Recorded separating sets for all removals
```

**Impact:** Prompts can include these constraints ‚Üí Better executor guidance

**Complexity:** Medium (requires new agent + prompt engineering)

**Algorithm-Agnostic:** ‚úÖ Yes (works for any algorithm)

---

### Proposal 2: Two-Level Prompt Generation (Abstract + Concrete)

**Goal:** Generate prompts with both **high-level instructions** and **low-level algorithmic checks**.

**Current Sequential Mode:**
```markdown
# TASK
Construct skeleton using CI tests

# STEP-BY-STEP
1. Build complete graph
2. Remove edges based on CI tests
```

**Enhanced Sequential Mode:**
```markdown
# TASK
Construct skeleton using conditional independence tests

# ALGORITHMIC REQUIREMENTS
- MUST start with complete graph (all 10 edges for 5 variables)
- MUST check EACH edge against premise CI statements
- MUST only remove edges with explicit CI justification
- MUST record exact separating set for each removal
- MUST test systematically (|S|=0, then |S|=1, then |S|=2, ...)

# STEP-BY-STEP INSTRUCTIONS
1. **Initialize Nodes**: Create nodes for all variables: {A, B, C, D, E}

2. **Initialize Complete Graph**: Create ALL possible edges:
   - A-B, A-C, A-D, A-E
   - B-C, B-D, B-E
   - C-D, C-E
   - D-E
   (Total: 10 edges)

3. **Systematic Edge Removal**:
   For EACH edge (X, Y) in the graph:
   a. Check premise for statement "X and Y are independent given S"
   b. Start with S = ‚àÖ (empty set)
   c. If found, remove edge and record Sepset[X-Y] = S
   d. Continue with larger conditioning sets if needed

4. **Record Separating Sets**:
   - For each removed edge, store the separating set
   - Use format "X-Y": [S] where S is the conditioning set

# SELF-VALIDATION CHECKLIST
Before returning output, verify:
- [ ] Started with exactly 10 edges
- [ ] Checked all edges against CI statements in premise
- [ ] Removed ONLY edges with explicit CI justification
- [ ] Recorded separating sets for ALL removed edges
- [ ] Final skeleton has ‚â§ 10 edges
- [ ] All remaining edges represent correlations from premise

# OUTPUT
Return JSON with keys: skeleton, sepsets
```

**Implementation:**
Update prompt agent system prompt in `multi_agent_planner.py`:

```python
# ADD to prompt template structure (line ~150):
"""
# ALGORITHMIC REQUIREMENTS
[Extract constraints from algorithm knowledge]
- MUST conditions (mandatory steps)
- MUST NOT conditions (prohibited actions)
- Systematic procedures to follow

# SELF-VALIDATION CHECKLIST
[List of verification steps before output]
- [ ] Verification point 1
- [ ] Verification point 2
...
"""
```

**Impact:**
- Significantly improves stage execution accuracy
- Reduces algorithmic errors
- Makes debugging easier (can check against checklist)

**Complexity:** Medium (prompt template modification + constraint extraction logic)

**Algorithm-Agnostic:** ‚úÖ Yes (every algorithm has requirements and validation points)

---

### Proposal 3: Execution Agent with Self-Validation

**Goal:** Make executor **verify its own work** before returning output.

**Current Executor System Prompt** (`executor.py:12-17`):
```python
system_prompt = """
You execute a specific stage of a decomposed reasoning workflow.
You will receive a rendered prompt template with context data.
Return ONLY valid JSON that matches the given output_schema exactly.
Focus only on the specific task described in the prompt.
Do not include explanations, markdown, or additional text - only the raw JSON.
"""
```

**Enhanced Executor System Prompt:**
```python
system_prompt = """
You are a precision algorithmic execution specialist with self-validation capabilities.

# EXECUTION PROTOCOL
1. **Understand**: Read and comprehend the task, requirements, and constraints
2. **Execute**: Perform the algorithmic steps systematically and carefully
3. **Validate**: Check your work against all requirements before returning
4. **Output**: Return JSON only if validation passes; otherwise, revise and retry

# SELF-VALIDATION REQUIREMENTS
Before returning your output, you MUST:
- Verify that all "ALGORITHMIC REQUIREMENTS" are satisfied
- Check all items in "SELF-VALIDATION CHECKLIST" (if provided)
- Confirm mathematical invariants hold
- Ensure output structure matches schema exactly
- Review for logical consistency

# CRITICAL RULES
- If you detect ANY error during validation, REVISE your output before returning
- If a requirement is unclear, use your best judgment but flag uncertainty
- Your output must be BOTH format-compliant AND algorithmically correct
- Return ONLY raw JSON, no explanations, markdown, or additional text

# ERROR DETECTION SIGNALS
Watch for these red flags in your work:
- Skipped steps in the instructions
- Missing data that should be present
- Outputs that seem too small or incomplete
- Mathematical inconsistencies
- Violations of stated constraints

# OUTPUT FORMAT
Return ONLY valid JSON matching the schema. No other text.
"""
```

**Impact:**
- Executor catches its own errors before output
- Reduces error propagation to downstream stages
- Improves overall pipeline reliability

**Complexity:** Low (just system prompt modification)

**Algorithm-Agnostic:** ‚úÖ Yes (self-validation applies to any task)

---

### Proposal 4: Stage Output Validation Layer

**Goal:** Add **automated validation** after each stage execution.

**Architecture:**
```python
async def run_stage_with_validation(
    stage: Stage,
    context: Dict[str, Any],
    validation_rules: Optional[Dict[str, Any]] = None,
    verbose: bool = True
) -> Dict[str, Any]:
    """Execute a stage and validate its output."""

    # Execute stage
    output = await run_stage(stage, context, verbose)

    # Validate output if rules provided
    if validation_rules and stage.id in validation_rules:
        validation_errors = validate_stage_output(
            stage_output=output,
            rules=validation_rules[stage.id],
            stage_id=stage.id
        )

        if validation_errors:
            logger.warning(f"‚ö†Ô∏è  Stage {stage.id} validation failed:")
            for error in validation_errors:
                logger.warning(f"   - {error}")

            # Strategy options:
            # Option 1: Retry with validation feedback
            # Option 2: Continue with warning
            # Option 3: Abort execution
            # For now, continue with warning

    return output


def validate_stage_output(
    stage_output: Dict[str, Any],
    rules: Dict[str, Any],
    stage_id: str
) -> List[str]:
    """Validate stage output against rules.

    Returns list of validation errors (empty if valid).
    """
    errors = []

    for key, constraints in rules.items():
        if key not in stage_output:
            errors.append(f"Missing required key: {key}")
            continue

        value = stage_output[key]

        # Type validation
        if "type" in constraints:
            expected_type = constraints["type"]
            if expected_type == "array" and not isinstance(value, list):
                errors.append(f"{key}: expected array, got {type(value)}")
            elif expected_type == "object" and not isinstance(value, dict):
                errors.append(f"{key}: expected object, got {type(value)}")

        # Length validation
        if isinstance(value, (list, dict)):
            if "min_length" in constraints:
                if len(value) < constraints["min_length"]:
                    errors.append(
                        f"{key}: length {len(value)} < minimum {constraints['min_length']}"
                    )
            if "max_length" in constraints:
                if len(value) > constraints["max_length"]:
                    errors.append(
                        f"{key}: length {len(value)} > maximum {constraints['max_length']}"
                    )
            if "exact_length" in constraints:
                if len(value) != constraints["exact_length"]:
                    errors.append(
                        f"{key}: length {len(value)} != required {constraints['exact_length']}"
                    )

        # Value validation
        if "allowed_values" in constraints:
            if value not in constraints["allowed_values"]:
                errors.append(
                    f"{key}: value {value} not in allowed values {constraints['allowed_values']}"
                )

        # Range validation
        if "min_value" in constraints and value < constraints["min_value"]:
            errors.append(f"{key}: value {value} < minimum {constraints['min_value']}")
        if "max_value" in constraints and value > constraints["max_value"]:
            errors.append(f"{key}: value {value} > maximum {constraints['max_value']}")

    return errors
```

**Validation Rules Generation:**

Update planning stage to generate validation rules alongside schemas:

```python
# Example validation rules for skeleton_identification stage
validation_rules = {
    "skeleton_identification": {
        "skeleton": {
            "type": "object",
            "properties": {
                "nodes": {
                    "type": "array",
                    "exact_length": 5,  # Must have 5 nodes for this sample
                },
                "edges": {
                    "type": "array",
                    "min_length": 1,    # Must have at least 1 edge
                    "max_length": 10,   # Can't have > 10 edges (complete graph)
                }
            }
        },
        "sepsets": {
            "type": "object",
            "min_length": 1,  # Should have removed at least 1 edge
        }
    }
}
```

**Integration Point:**

In `executor.py`, modify `run_plan()`:

```python
async def run_plan(
    plan: Plan,
    initial_context: Dict[str, Any],
    validation_rules: Optional[Dict[str, Any]] = None,
    verbose: bool = True
) -> Dict[str, Any]:
    """Execute a complete plan with optional validation."""

    context = dict(initial_context)

    for stage in plan.stages:
        # Execute with validation
        stage_output = await run_stage_with_validation(
            stage, context, validation_rules, verbose
        )

        # Update context
        for key in stage.writes:
            context[key] = stage_output[key]

    return context
```

**Impact:**
- Catches errors immediately after they occur
- Prevents error propagation
- Provides clear debugging information

**Complexity:** High (requires validation rule generation + execution integration)

**Algorithm-Agnostic:** ‚ö†Ô∏è Partially (validation rules are domain-specific but framework is generic)

---

### Proposal 5: Plan Caching with Variation

**Goal:** Generate and test **multiple plan variants** instead of single plan.

**Current Limitation:**
- Plan caching reuses same plan for similar samples
- If plan is wrong ‚Üí all cached samples fail the same way

**Enhanced Approach:**

```python
async def generate_plan_variants(
    task_description: str,
    algorithm_knowledge: str,
    num_variants: int = 3,
    strategies: List[str] = ["detailed", "constraint-heavy", "validation-focused"]
) -> List[Plan]:
    """Generate multiple plan variants with different strategies."""

    plans = []

    for strategy in strategies[:num_variants]:
        # Modify planning prompt based on strategy
        enhanced_task = add_strategy_context(task_description, strategy)

        # Generate plan with strategy
        plan, metadata = await planner.generate_plan(
            enhanced_task,
            algorithm_knowledge,
            use_sequential=True  # Always use sequential for variants
        )

        plan.metadata = {"strategy": strategy}
        plans.append(plan)

    return plans


def add_strategy_context(task_description: str, strategy: str) -> str:
    """Enhance task description with strategy-specific guidance."""

    strategy_prompts = {
        "detailed": "\nFOCUS: Generate extremely detailed prompts with step-by-step breakdowns.",
        "constraint-heavy": "\nFOCUS: Emphasize algorithmic constraints and requirements in every stage.",
        "validation-focused": "\nFOCUS: Include extensive self-validation checklists at each stage."
    }

    return task_description + strategy_prompts.get(strategy, "")


async def run_batch_with_plan_rotation(
    plans: List[Plan],
    samples: List[Sample],
    initial_context_fn: Callable
) -> List[ExperimentResult]:
    """Run batch with different plans and track which performs best."""

    results_by_plan = {i: [] for i in range(len(plans))}

    for idx, sample in enumerate(samples):
        # Rotate through plans
        plan_idx = idx % len(plans)
        plan = plans[plan_idx]

        # Run experiment
        context = initial_context_fn(sample)
        result = await run_plan(plan, context)

        # Track result
        results_by_plan[plan_idx].append({
            "sample_idx": idx,
            "result": result,
            "plan_strategy": plan.metadata["strategy"]
        })

    # Analyze which plan performed best
    analyze_plan_performance(results_by_plan, plans)

    return results_by_plan


def analyze_plan_performance(
    results_by_plan: Dict[int, List],
    plans: List[Plan]
):
    """Analyze and report which plan variant performed best."""

    print("\nüìä Plan Variant Performance Analysis")
    print("=" * 60)

    for plan_idx, results in results_by_plan.items():
        strategy = plans[plan_idx].metadata["strategy"]
        accuracy = calculate_accuracy(results)

        print(f"\nStrategy: {strategy}")
        print(f"  Samples: {len(results)}")
        print(f"  Accuracy: {accuracy:.2%}")

    # Identify best performing strategy
    best_plan_idx = max(
        results_by_plan.keys(),
        key=lambda i: calculate_accuracy(results_by_plan[i])
    )

    print(f"\nüèÜ Best Strategy: {plans[best_plan_idx].metadata['strategy']}")
```

**Impact:**
- Increases robustness by testing multiple approaches
- Helps identify which prompt strategies work best
- Provides data for future prompt engineering

**Complexity:** High (requires multi-plan orchestration + analysis)

**Algorithm-Agnostic:** ‚úÖ Yes (works for any algorithm/task)

---

### Proposal 6: Failure Analysis & Auto-Improvement Loop

**Goal:** System learns from failures to improve future plans.

**Architecture:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Run Batch   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Collect Failures‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Analyze Failures ‚îÇ ‚óÑ‚îÄ‚îÄ LLM Agent
‚îÇ (Which stages?)  ‚îÇ
‚îÇ (What errors?)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Generate Improvements ‚îÇ ‚óÑ‚îÄ‚îÄ LLM Agent
‚îÇ (Prompt updates)      ‚îÇ
‚îÇ (Constraint additions)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Update Plan     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Re-run Failed   ‚îÇ
‚îÇ Samples         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Implementation:**

```python
class FailureAnalyzer:
    """Analyzes failures and generates improvement suggestions."""

    def __init__(self):
        self.analysis_agent = Agent(
            "openai:o3-mini",
            output_type=str,
            system_prompt="""
You are a failure analysis specialist for algorithmic execution pipelines.

# TASK
Analyze failed samples to identify systematic issues and improvement opportunities.

# ANALYSIS DIMENSIONS
1. **Stage-Level Errors**: Which stages produced incorrect outputs?
2. **Algorithmic Errors**: What specific algorithmic mistakes were made?
3. **Constraint Violations**: Which requirements were violated?
4. **Pattern Recognition**: Are there common failure modes across samples?

# OUTPUT
Provide structured analysis with:
- Root cause identification
- Affected stages
- Specific errors observed
- Improvement recommendations

Be specific and actionable in your recommendations.
"""
        )

        self.improvement_agent = Agent(
            "openai:o3-mini",
            output_type=str,
            system_prompt="""
You are a prompt improvement specialist.

# TASK
Given failure analysis, generate specific prompt improvements.

# INPUT
- Current stage prompts
- Failure analysis (what went wrong)
- Algorithm knowledge

# OUTPUT
For each affected stage, provide:
1. Specific prompt additions (new requirements, constraints)
2. Enhanced validation checklists
3. Clearer step-by-step instructions

Focus on making prompts more robust against observed failure modes.
"""
        )

    async def analyze_failures(
        self,
        failed_samples: List[ExperimentResult],
        plan: Plan,
        dataset: pd.DataFrame
    ) -> str:
        """Analyze failures and identify patterns."""

        # Construct analysis prompt
        failure_details = self._format_failures(failed_samples, dataset)
        plan_details = self._format_plan(plan)

        prompt = f"""
Analyze these failures to identify systematic issues:

FAILED SAMPLES:
{failure_details}

CURRENT PLAN:
{plan_details}

Identify:
1. Which stages are producing errors?
2. What types of errors are occurring?
3. Are there common patterns?
4. What improvements would help?
"""

        result = await self.analysis_agent.run(prompt)
        return result.output

    async def generate_improvements(
        self,
        failure_analysis: str,
        plan: Plan,
        algorithm_knowledge: str
    ) -> Dict[str, str]:
        """Generate improved prompts for affected stages."""

        prompt = f"""
Generate prompt improvements based on this failure analysis:

FAILURE ANALYSIS:
{failure_analysis}

CURRENT PLAN:
{json.dumps([{
    "id": s.id,
    "prompt": s.prompt_template
} for s in plan.stages], indent=2)}

ALGORITHM KNOWLEDGE:
{algorithm_knowledge}

For each stage that needs improvement, provide enhanced prompt.
"""

        result = await self.improvement_agent.run(prompt)

        # Parse improved prompts
        improved_prompts = self._parse_improvements(result.output)

        return improved_prompts

    def _format_failures(
        self,
        failures: List[ExperimentResult],
        dataset: pd.DataFrame
    ) -> str:
        """Format failure details for analysis."""

        details = []
        for failure in failures[:5]:  # Analyze first 5 failures
            sample = dataset.iloc[failure.sample_idx]
            details.append(f"""
Sample {failure.sample_idx}:
  Input: {failure.sample_input[:200]}...
  Expected: {failure.expected}
  Predicted: {failure.predicted}
  Error: {failure.error or 'Incorrect prediction'}
""")

        return "\n".join(details)

    def _format_plan(self, plan: Plan) -> str:
        """Format plan for analysis."""

        stages = []
        for stage in plan.stages:
            stages.append(f"""
Stage: {stage.id}
  Reads: {stage.reads}
  Writes: {stage.writes}
  Prompt (first 300 chars): {stage.prompt_template[:300]}...
""")

        return "\n".join(stages)

    def _parse_improvements(self, improvement_text: str) -> Dict[str, str]:
        """Parse improvement suggestions into stage_id -> improved_prompt mapping."""

        # Simple parsing logic (can be enhanced)
        # For now, return empty dict (to be implemented)
        return {}


async def run_with_auto_improvement(
    config: ExperimentConfig,
    max_iterations: int = 2
) -> BatchResults:
    """Run batch experiments with auto-improvement loop."""

    analyzer = FailureAnalyzer()
    planner = MultiAgentPlanner()

    # Initial run
    runner = BatchExperimentRunner(config)
    runner.load_dataset()
    results = await runner.run_batch()

    # Improvement loop
    for iteration in range(max_iterations):
        # Check if improvement needed
        if results.accuracy >= 0.90:  # Good enough
            print(f"‚úÖ Accuracy {results.accuracy:.2%} - no improvement needed")
            break

        print(f"\nüîÑ Improvement Iteration {iteration + 1}")

        # Analyze failures
        failures = [r for r in results.individual_results if not r.is_correct]

        if not failures:
            break

        print(f"üìä Analyzing {len(failures)} failures...")

        analysis = await analyzer.analyze_failures(
            failures,
            runner.cached_plan,  # Assume plan is cached
            runner.dataset
        )

        print(f"üìù Failure Analysis:\n{analysis[:500]}...")

        # Generate improvements
        improvements = await analyzer.generate_improvements(
            analysis,
            runner.cached_plan,
            runner.cached_knowledge
        )

        if not improvements:
            print("‚ö†Ô∏è  No improvements generated, stopping")
            break

        print(f"‚ú® Generated improvements for {len(improvements)} stages")

        # Update plan with improvements
        updated_plan = apply_improvements(runner.cached_plan, improvements)

        # Re-run failed samples with updated plan
        print(f"üîÑ Re-running {len(failures)} failed samples...")

        rerun_results = await rerun_samples(
            [f.sample_idx for f in failures],
            updated_plan,
            runner.dataset
        )

        # Update overall results
        results = merge_results(results, rerun_results)

        print(f"üìä New Accuracy: {results.accuracy:.2%}")

    return results
```

**Impact:**
- Automatically improves plans based on failures
- Reduces manual debugging time
- Scales learning across multiple samples

**Complexity:** Very High (requires sophisticated analysis + prompt modification logic)

**Algorithm-Agnostic:** ‚úÖ Yes (failure analysis works for any domain)

---

## Implementation Roadmap

### Phase 1: Quick Wins (1-2 days)

**Priority P0 - Immediate Impact**

#### Task 1.1: Update Prompt Agent System Prompt
- **File:** `self_planned/src/plan/multi_agent_planner.py:118-172`
- **Changes:**
  - Change line 165: Remove "not algorithmic theory" constraint
  - Add "ALGORITHMIC REQUIREMENTS" section to template
  - Add "SELF-VALIDATION CHECKLIST" section to template
- **Expected Impact:** 30-40% improvement in prompt quality

#### Task 1.2: Update Executor System Prompt
- **File:** `self_planned/src/execute/executor.py:12-17`
- **Changes:**
  - Replace with enhanced system prompt (see Proposal 3)
  - Add self-validation protocol
  - Add error detection guidance
- **Expected Impact:** 20-30% reduction in execution errors

#### Task 1.3: Test on Sample 264
- Run updated pipeline on sample 264
- Verify improvements
- Document results

**Deliverables:**
- ‚úÖ Updated `multi_agent_planner.py`
- ‚úÖ Updated `executor.py`
- ‚úÖ Test results document

---

### Phase 2: Enhanced Knowledge (3-5 days)

**Priority P1 - Foundation Improvement**

#### Task 2.1: Add Constraints Agent to Knowledge Extraction
- **File:** `self_planned/src/knowledge/extractor.py`
- **Changes:**
  - Add new `constraints_agent` to `EnhancedKnowledgeExtractor`
  - Define extraction for: preconditions, postconditions, invariants
  - Update synthesis to include constraints

#### Task 2.2: Update Knowledge Output Format
- Extend `<CANONICAL_STAGES>` with constraint fields
- Add `<STAGE_VALIDATION>` section

#### Task 2.3: Update Planning to Use Constraints
- Modify prompt generation to incorporate constraints
- Test on multiple algorithms

**Deliverables:**
- ‚úÖ Enhanced knowledge extractor
- ‚úÖ Constraint-aware prompts
- ‚úÖ Test results on 3+ algorithm types

---

### Phase 3: Validation Layer (5-7 days)

**Priority P1 - Robustness**

#### Task 3.1: Implement Validation Rule Generation
- **New File:** `self_planned/src/plan/validation_rules.py`
- Generate validation rules during planning
- Store alongside schemas

#### Task 3.2: Implement Stage Output Validation
- **File:** `self_planned/src/execute/executor.py`
- Add `run_stage_with_validation()`
- Add `validate_stage_output()`
- Integrate into `run_plan()`

#### Task 3.3: Test Validation System
- Run on known failing samples
- Verify error detection
- Tune validation thresholds

**Deliverables:**
- ‚úÖ Validation rule generation
- ‚úÖ Validation execution layer
- ‚úÖ Validation effectiveness report

---

### Phase 4: Advanced Features (7-10 days)

**Priority P2 - Optimization**

#### Task 4.1: Implement Plan Variants
- **New File:** `self_planned/src/plan/plan_variants.py`
- Generate multiple plan variants
- A/B testing framework

#### Task 4.2: Implement Failure Analyzer
- **New File:** `self_planned/src/analysis/failure_analyzer.py`
- Failure pattern detection
- Improvement suggestion generation

#### Task 4.3: Auto-Improvement Loop
- **File:** `self_planned/src/execute/batch_experiments.py`
- Add improvement iteration logic
- Test on batch experiments

**Deliverables:**
- ‚úÖ Plan variant system
- ‚úÖ Failure analyzer
- ‚úÖ Auto-improvement loop
- ‚úÖ Comparative performance report

---

## Expected Impact

### Quantitative Metrics

| Metric | Baseline | After Phase 1 | After Phase 2 | After Phase 3 | After Phase 4 |
|--------|----------|---------------|---------------|---------------|---------------|
| **Accuracy** | ~60-70% | ~70-75% | ~75-80% | ~80-85% | ~85-90% |
| **Stage Error Rate** | ~30% | ~20% | ~15% | ~10% | ~5% |
| **Debugging Time** | High | Medium | Medium | Low | Very Low |
| **False Positives** | ~20% | ~15% | ~12% | ~8% | ~5% |
| **False Negatives** | ~15% | ~12% | ~10% | ~7% | ~5% |

*Note: These are estimates based on typical LLM reasoning improvements. Actual results may vary.*

---

### Qualitative Improvements

#### After Phase 1 (Quick Wins):
- ‚úÖ Executor self-validates before output
- ‚úÖ Prompts include algorithmic requirements
- ‚úÖ Errors caught earlier in pipeline
- ‚úÖ Better diagnostic information

#### After Phase 2 (Enhanced Knowledge):
- ‚úÖ Prompts include mathematical constraints
- ‚úÖ Clear validation criteria for each stage
- ‚úÖ Better handling of edge cases
- ‚úÖ More robust across algorithm types

#### After Phase 3 (Validation Layer):
- ‚úÖ Automatic error detection after each stage
- ‚úÖ Prevent error propagation
- ‚úÖ Clear failure diagnosis
- ‚úÖ Reduced debugging time

#### After Phase 4 (Advanced Features):
- ‚úÖ Multiple solution strategies tested
- ‚úÖ System learns from failures
- ‚úÖ Automatic prompt improvement
- ‚úÖ Scalable quality improvement

---

### Impact on Sample 264

**Current Execution:**
```
Stage 1: Wrong skeleton (4 edges) ‚Üí No validation
Stage 2: No colliders ‚Üí Builds on wrong skeleton
Stage 3: Wrong orientation ‚Üí Compounds error
Stage 4: Wrong decision ‚Üí Final failure
```

**After Phase 1 (Quick Wins):**
```
Stage 1: Executor self-validates
  - "I only have 4 edges, but started with 10"
  - "Let me check my work..."
  - Revises to proper skeleton (6-7 edges)
Stage 2: Correct colliders identified
Stage 3: Correct orientations
Stage 4: Correct decision ‚úÖ
```

**After Phase 3 (Validation Layer):**
```
Stage 1: Outputs 4 edges
  ‚Üì
Validation Layer:
  - "ERROR: 4 edges < minimum expected (5)"
  - "ERROR: Did not remove enough edges systematically"
  ‚Üì
Retry Stage 1 with validation feedback
  ‚Üì
Correct output ‚úÖ
```

---

## Maintenance & Monitoring

### Continuous Monitoring

**Metrics to Track:**
- Stage-level error rates
- Validation failure frequency
- Retry rates
- Execution time per stage
- Overall accuracy trends

**Logging Enhancements:**
```python
# Add to executor.py
logger.log_stage_metrics({
    "stage_id": stage.id,
    "execution_time": execution_time,
    "output_size": len(json.dumps(stage_output)),
    "validation_passed": validation_passed,
    "retry_count": retry_count
})
```

### Quality Assurance

**Regular Testing:**
- Run test suite on known samples weekly
- Compare against baseline accuracy
- Identify regression

**A/B Testing:**
- Test new prompt strategies
- Compare against production prompts
- Roll out improvements gradually

---

## Conclusion

### Summary

The failure of Sample 264 revealed a **fundamental gap** between high-level algorithm knowledge and low-level execution precision. The current pipeline successfully identifies what needs to be done but lacks the mechanisms to ensure it's done correctly.

### Core Improvements

1. **Prompt Quality** (P0): Add algorithmic constraints and self-validation
2. **Executor Self-Validation** (P0): Make LLM verify its own work
3. **Knowledge Enhancement** (P1): Extract operational constraints
4. **Validation Layer** (P1): Automated output validation
5. **Plan Variants** (P2): Test multiple approaches
6. **Auto-Improvement** (P2): Learn from failures

### Maintaining Algorithm-Agnostic Design

All proposed improvements maintain the system's algorithm-agnostic nature:
- ‚úÖ Work for any algorithm (PC, Dijkstra, MCMC, etc.)
- ‚úÖ No hardcoded domain logic
- ‚úÖ Generic validation framework
- ‚úÖ Transferable across tasks

### Next Steps

1. **Immediate (This Week):** Implement Phase 1 (Quick Wins)
2. **Short Term (2-3 Weeks):** Implement Phase 2 (Enhanced Knowledge)
3. **Medium Term (1-2 Months):** Implement Phase 3 (Validation Layer)
4. **Long Term (2-3 Months):** Implement Phase 4 (Advanced Features)

### Success Criteria

- ‚úÖ Sample 264 passes after Phase 1
- ‚úÖ Overall accuracy improves by 15-20% after Phase 2
- ‚úÖ Stage error rate < 10% after Phase 3
- ‚úÖ System demonstrates learning capability in Phase 4

---

**Document Version:** 1.0
**Last Updated:** 2025-12-15
**Author:** Claude (Anthropic)
**Review Status:** Ready for Implementation
